\chapter{Discussion}

simulation of historical order matching

data samples chosen during feature construction

data samples chosen during evaluation

advantages and disadvantages of q-learning and dqn

model parameters

\chapter{Conclusion and Future Work}
\label{chap:conclusion}

The aim of this thesis was to mitigate the impact of the order placement problem with deep reinforcement learning.
Over the course of working on this project it has been understood that, in order to propose a solution to this problem, multiple concepts are required to be understood and developed.
Therefore, this thesis was structured in a way in which each chapter contributes constructively and in part towards the translation of the order placement problem into the reinforcement learning context and finally allows to pursue experiments which state the effectiveness of our setup.
This chapter summarizes the contributions made in this thesis and revisits the research questions with respect to the corresponding contributions made.
Subsequently, recommendations for future work to be done is given.
Finally, a motivation is stated which supports the application of the techniques developed in real world practices.

\section{Summary of Contributions}

This work aims to take a step towards answering the important question of how one can optimize the limit order placement on financial exchanges using deep reinforcement learning.
Previous work in this field, by Kearns et al., who have studied the behavior of order placement and order execution\cite{nevmyvaka2005electronic} and developed a reinforcement learning strategy\cite{nevmyvaka2006reinforcement} for the purposes of optimization, has been extended and applied to the cryptocurrency field.

In this work, raw market data of the Bitcoin/US-Dollar trading pair was analyzed and an investigation of the activity of market participants has shown that patterns emerge while traders create and cancel orders in financial markets.
Based on these findings, two features where designed which incorporate the found patterns in form of a window of 1) historical order book states and 2) historical trades, respectively.
Furthermore, key components which are required in order to simulate the limit order placement on historical market data were identified and developed.
This processes involved the construction of a limit order book data structure as well as an experimental and simplified match engine that enables to emulate a local broker that can match orders using the historical order book.
Those mechanisms were incorporated into a reinforcement learning environment which allows to make the translation of the order placement problem into the reinforcement learning context.
The environment developed is configurable and therefore flexible enough to enable investigations with the previously constructed features as well a variety of agents.
In addition, the environment restricts the agents, which intend to buy or sell a defined inventory of assets within a given time horizon, 
with the use of a discrete action space as well as discrete steps that can be taken during an epoch.
Next to the environment, two implementations of reinforcement learning agents were developed.
A Q-Learning agent serves as the learner when no market variables are provided and a Deep Q-Network agent is developed to handle the features previously mentioned.

Given the fact that previous research has not yet been applied to the cryptocurrency market and therefore no optimization performance figures are provided to which the approach of this work can be compared to, an evaluation procedure was carried out as part of this work.
Thereby, two real world market data sets were chosen as well as artificially generated limit order books.
The muti-step procedure involved an empirical investigation of the expected costs of an optimally placed limit order as well as the costs of an immediate purchase or sale using a market order.
Furthermore, the expected costs were taken as a benchmark for the Q-Learning agent as well as the DQN agent under the application of both aforementioned feature types.

The results have shown that the Q-Learning agent was not able to take advantage of market price movements that came in favour of the order placement process.
Nevertheless, the Q-Learner was able to reduce the costs when the market price development did not come in favor of either buying or selling assets and outperformed all considered agent setups.
The DQN agent, on the other hand, was able to learn a policy that allows to optimize the order placement when market conditions came in favor of making a purchase or sale.
However, this agent performed worse than the expected costs of a market order when conditions were not ideal.
Furthermore, it has been shown that the application of feature II (a sequence of historical trades) in an overall better policy than the application of feature I (a window of historical order book states).
Finally, the limitations of the deep reinforcement learning approach was investigated.
Results have shown that the agent is capable of finding a near-optimal policy on artificial market data, and therefore implies that non-stationary as well as noisy market data prevented the agent to achieve similar results.
In addition it has been discovered that, under the application of real world market data, the agent can be exposed to severe conditions, in form of wide spreads and absence of liquidity, which prevent the agent from applying an optimal policy.

\section{Revisiting the Research Questions}


\section{Future Work}

\section{The application in real world practices}