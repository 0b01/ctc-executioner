\chapter{General conclusions and discussion}
\label{chap:discussion}

This thesis serves to determine how deep reinforcement learning can be used to optimize the limit order placement problem.
Over the course of working on this project it has been understood that, in order to propose a solution to this problem, concepts from the financial as well as the machine learning context are required to be understood and developed.
Therefore, this thesis was structured in a way in which each chapter contributes constructively and in part towards the translation of the order placement problem into the reinforcement learning context and finally allows to pursue experiments which state the effectiveness of our setup.
The required components involved were introduced in Chapter \ref{chap:preliminaries}; the data, including the features constructed therefrom, were elaborated in Chapter \ref{chap:data}.
In Chapter \ref{chap:setup} a reinforcement learning environment and two implementations of agents were developed, by which inspiration and knowledge was acquired from the related work presented in Chapter \ref{chap:related-work}.
Ultimately, the setup was evaluated in Chapter \ref{chap:analysis} and the found results show that the DQN agent was able to outperform the expected volume weighted average price for market orders by \$14.18 for buying 1.0 BTC within a time horizon of 100 seconds.
However, the agent failed to outperform a market order by \$15.0 for selling 1.0 BTC within the equivalent time horizon.

This chapter discusses and revisits the research questions with respect to the corresponding contributions made and thereby mentions strengths and limitations of this work.
Furthermore, a summary of the contributions made is provided.
Finally, future work is suggested and a motivation is stated which supports the application of the techniques developed to be used in real world practices.

\section{Findings with regard to the research questions}

In Chapter \ref{chap:introduction} (Section \ref{sec:intro-rq}) the purpose of this thesis was carried out by stating the research objectives.
This section aims to revisit these research questions, conclude what has been achieved in their regards, and discuss these findings in greater detail.

\subsection{RQ 1.1: Which historical market data patterns drive market participants to buy or sell assets, and how can these patterns be incorporated into features used by a deep reinforcement learning agent?}

    Historical market event data was investigated in Chapter \ref{chap:data}. 
    Thereby, patterns were found which give insight into how market participants positioned their orders, with respect to price and size. 
    It was shown that the price movement was likely to be due to (1) an imbalance between bid and ask orders; (2) a distinctive way of posting or canceling orders; and (3) consecutive or impulsive trades.
    Furthermore, hypotheses were formed based on these findings and two features were constructed which incorporate emerging data that was foundation for the patterns found.
    Namely, a window of historical limit order book states (feature I) and historical trade events (feature II).
    \\
    \\
The first attempt to find patterns in market data was made by investigating the relationship between trade activity (volume) and price change and volatility, as proposed by Karpoff et al. \cite{karpoff1987relation}.  
As traders submit limit orders to buy (sell), they impact the bid (ask) volumes of the  limit order book and thereby gives us a view of the tradersâ€™ intentions and would allow  to foresee the direction of the upcoming price changes.
To quantify this intent, we looked at the difference between the bid and ask volume, called order imbalance, as shown in Section \ref{sec:data-hypthesis-order-volume}.
Even though imbalance was clearly present, the high volatility in the Bitcoin/USD market prevented us from finding obvious correlation to the direction of future price movements.
This gave reasons to believe that the data had to be investigated on the raw market event level\cite{kane2011analyzing}.
The visualization of market events occurred over time, in a volume map, allowed to detect and associate patterns to future price changes (Section \ref{sec:data-hypthesis-order-trade-volume-time}).
It was found that orders submitted and cancelled, as well as the trades generated, had an impact on the future price movement.
However, in order to determine whether or not this information is beneficial regarding the optimization of limit order placement, a feature had to be constructed with which an agent can attempt to learn a policy.
Hand-crafting features such as measuring the change in volume would have excluded important information regarding the price level and frequency of the orders posted.
Recent advances in deep reinforcement learning have demonstrated the ability to learn directly from pixel frames\cite{mnih2013playing} and raw sensory data\cite{mnih2015human} occurred over time.
We followed the same principles and constructed features which reflect (1) the order book states and (2) the trades that have occurred over time.
As a result, the constructed features will contain the entire information and as soon as a new market event is published a new sample can be added to the feature set.
One of the difficulties with these constructed features was to determine with appropriate size of the data window to be considered.
Entropy and correlation of order prices and sizes suggests that at least a window of 30 order book states should be considered.
While this being no guaranteed for optimal performance, we tested window sizes up to 60 states and found no improvements during a backtest.

\subsection{RQ 1.2: How should one design a reinforcement learning environment and agents, in the context of order placement?}

    The challenging task of mapping the order placement problem into the reinforcement learning context demanded an investigation of components involved in the financial setting as well as in a reinforcement learning setting and was described in Chapter \ref{chap:preliminaries}.
    In Chapter \ref{chap:setup}, these components were combined and a reinforcement learning environment was built.
    The environment built configures a discrete action space with which an agent can explore the state space that is determined by the underlying historical market data.
    The reward function in use represents the outcome of the matching process of an order submitted.
    Since most of the capabilities were made available within the environment, the required complexity of the agents could be reduced drastically.
    Two agents where developed with the purposes of learning with and without market features.
    \\
    \\
    The key components which ultimately enabled to build a reinforcement learning environment that emulates a local broker and therefore allows to simulate limit order placement using historical orders book, are, the order book (Section \ref{sec:order-book}) and the match engine (Section \ref{sec:match-engine}).
    The difficulties that arose during the development of these components are related to performance and inevitable approximations required during the matching process.
    The vast amount of market events led the matching process to be become unbearably slow.
    Improvements were made by using caching and indexing mechanisms but it remains as a limitation of this project and ultimately limited the scope of the evaluation that was proceeded (the run-time for an agent to train over 5000 epochs took more than 4 hours).
    Undoubtedly, improvements could be done by using techniques such as proposed in \cite{barazzutti2016exploiting}.
    However, this was out of scope in this project as the focus was laid to the general principle of how to map the order placement problem into the reinforcement learning context, rather than building an efficient matching engine.
    A more serious limitation when matching orders using historical order books is the absence of market participants, who could have 1) entered or 2) left the market upon placing an order during the simulation.
    Participants who would enter the market would likely be favorable to us as they would act as potential buyers and sellers and therefore provide liquidity.
    Participants who leave the market would introduce a slight disadvantage as there would be less liquidity.
    Therefore the matching process is strictly speaking an estimation of what would have happened in the past.
    However, since only up to 1.0 BTC were used for buying and selling, which is insignificant in terms of having a market impact\cite{hautsch2012market}, we considered the approximation of the matching process as sufficiently correct.
    Otherwise, the only way to overcome this limitation would be to train and test on live market data and therefore make real purchases and sales.
    
    The environment built in this work confirms that reinforcement learning is a suitable for the optimization of the limit order placement problem.
    The end-to-end learning process, which is known to be successful in other domains\cite{amodei2016deep, mnih2015human, mnih2013playing}, allowed us to regard the underlying order matching process as a black-box.
    This in turn allowed the environment itself to be kept relatively simple.
    Moreover, with our setup the agent can improve its policy with the use of a reward function whose outcome is the volume weighted average price that is directly determined by the outcome of the matching process.
    Furthermore, the environment does not only support the use of multiple agents in a standardized way but also allowed us to estimate the expected rewards for certain actions taken by the agent.
    The fact that the limit order placement process is non-trivial and can be attempted in many ways, we were required to equip the environment with a variety of configuration parameters, as defined in Section \ref{setup:parameters}.
    While this provides the ability to define evaluation setups very precisely, it also introduces the inevitable obligation to limit the scope of how the problem is attempted to be optimized.
    We have decided to define the discrete time step size $\Delta{t}=10 seconds$, which represents the duration of how long an order remains in the order book for each step taken by the agent and therefore limits the number of steps to be taken throughout one epoch to $\frac{H}{\Delta{t}}=10$.
    With the empirical investigation (Section \ref{sec:eval-empirical}) we have shown that a placement of 10 seconds diverges from the expected rewards received for placing the order for a total of 100 seconds.
    Naturally, if the agent decides for each step to choose the same action, the result is equivalent to a placement over 100 seconds.
    Therefore, the benefit of the time step parameter is the agents ability to intercept and change the action accordingly, in case a sudden change in the market data suggests to place the order at a difference price level.
    Investigations were made with time step parameters smaller than 10 seconds (3s, 5s) and no improvements could be made.
    The only difference was an increased training time as the agent had more steps to take for one epoch.
    Another important parameter is the size of the action space, which we defined to be consisting of 50 negative and 50 positive actions that result in a total action space size of $|A|=101$.
    The empirical investigation in Section \ref{sec:eval-empirical} has confirmed that this is indeed the range of actions which have most influence in the posting of order over a time horizon of 100 seconds.
    However, the limitations found in Section \ref{sec:eval-dqn-limitations} have shown that there are rare occasions where the defined action space is too small and therefore the agent is unable to place the order at a price level close to the offers of potential buyers or sellers.
    Increasing the already rather large action space would certainly allow the agent to place orders at the appropriate price levels.
    However, experiments with $|A|=201$ have shown that this comes with the cost of increased training required as well as a greater potential for misplacing orders, and therefore performance could not be improved.
    Alternatively, the action step size, which is currently \$0.10, would allow to increase the price range at which to place orders by maintaining equal action space size.
    However, this implies that the agent would make more coarse decisions in terms of the price level at which to place the order and therefore potential optimal placements would be out of reach.
    Our investigations have shown that no improvements could be made with $\Delta{a}=\$0.50$ and $\Delta{a}=\$1.00$.

    The Q-Learning and DQN agents developed are similar in terms of the reward function in use that underlies the principles of the Bellman equation (Eq. \ref{eq:bellman}).
    However, much complexity is added to the DQN agent, such as the use of an \textit{experience replay} and most importantly the use of a neural network that serves as \textit{value function approximator}.
    While the Q-Learning agent can be suitable when no market variables, or an approximation thereof, are applied, it was found not to be an appropriate choice for the purpose of optimizing the limit order placement problem in general.
    Market features, such as the ones constructed in Chapter \ref{chap:data}, would not provide any benefit when being applied to the Q-Learning agent.
    Reason for this is that Q-Learning makes use of a look-up table which records a combination of all visited observation states and chosen actions.
    Since the observation states are derived from market data at a given point in time, most observations states become unique and thus learning time is expected to scale exponentially with the size of the state space\cite{whitehead1991complexity}.
    To overcome this limitation, market features would have to be approximated drastically, as demonstrated in \cite{nevmyvaka2006reinforcement}.
    Contrarily, and for reasons stated above, the DQN proofed to be a suitable in this regard.
    We briefly investigated the architecture of the neural network briefly, including the experimentation with long short-term memory\cite{gers1999learning} as well as a 2-layer perceptron.
    Our investigations have shown that the CNN architecture proposed in \cite{mnih2015human} performed best and was chosen for further evaluations in Chapter \ref{chap:analysis}.
    Clearly, there is room for improvements with regard to parameter tuning, which we neglected in this work due to the prioritization of the architectural aspects of the reinforcement learning setup and therefore suggest this as future work.

\subsection{RQ 1.3: How can one evaluate a reinforcement learning agent in the context of order placement?}

    The fact that previous research has not yet been applied to the cryptocurrency market and therefore no optimization performance figures exist, to which the approach of this work can be compared to, an evaluation procedure was worked out as part of this thesis (Section \ref{sec:analysis-procedure}).
    Therefore, two real world market data sets were chosen as well as artificially generated limit order books.
    The multi-step procedure involved an empirical investigation of the expected costs of an optimally placed limit order as well as the costs of an immediate purchase or sale using a market order.
    Furthermore, the expected costs were taken as a benchmark for the Q-Learning agent as well as the DQN agent under the application of both aforementioned feature types.
    In addition, an evaluation mechanism was built into the reinforcement learning environment which allows to investigate the steps taken by an agent while proceeding a training or testing epoch.
    This allowed to detect the limitations of the agent as well as the environment in greater detail.
    \\
    \\
    The way in which order placement was simulated is heavily based on suggestions made regarding backtesting by Pedro et al. \cite{de2018advances}.
    Particularly, the selection of random order book state that serves as the starting point of an epoch the agent initializes drastically reduces the chances for possible over-fitting.
    With this in mind, we considered data sets which show different market price constellations and furthermore developed a way to create artificial order books to train and test the agent on.
    It is obvious that only by considering an infinite amount of data sets a completely generalized statement about the optimization capabilities can be made, however, and as mentioned earlier, this comes at the cost of time and computational resources.
    However, our novel approach to generate artificial data sets has proven to be of substantial value since the possible optimization factor is known prior the validation of the model and processing is much faster.
    
    The empirical investigation step executed prior the training of a policy has provided baseline performance results.
    The expected costs for market and limit orders where determined and the results have shown that the expected behaviour for limit order placement is similar to what was proposed by Kearns et al. \cite{nevmyvaka2005electronic} who worked with traditional stocks.
    However, we have shown more detailes of the behaviour with respect to the given time horizon for the placements of orders.
    The authors of \cite{nevmyvaka2005electronic} stated that, overall, the optimal price at which to place an order is just below the spread price.
    This statement is true for the cryptocurrency market as well, however, only under the condition that the time horizon is chosen either very small ($H\leq10 seconds$) or rather large ($H>15 minutes$).
    Otherwise, for example when $H=100 seconds$, we have demonstrated that the optimal price at which to place an order changes drastically depending on the fluctuations of the market price.
    Namely, for buying, limit orders should be placed deep in the order book when market prices are falling, and high in the book when the price is rising.
    Contrarily, for selling, orders should be place high in the book when the market price is falling and low in the book when the price is rising.
    These insights not only provided knowledge of how to optimize the placement of orders but also generated means to compare the performance achieved by reinforcement learning agents.
    
    We further reflect that average rewards provided much more understanding about the performance of the reinforcement learners, as opposed to the additionally derived average actions chosen.
    An average action, consisting of up to 10 values (equivalent to the maximum number of steps for one epoch) from the range of 101 total actions, certainly provides knowledge about the tendency at which price level the learners attempts to place orders.
    However, an observation and the analysis of the specific sequence of actions chosen is suggested to be derived in future work.
    
\subsection{RQ 1.4: In which way do the previously constructed features enable a reinforcement learning agent to improve the way it places orders?}

    The aforementioned evaluation procedure was used to determine the efficiency of the constructed features applied to the DQN agent.
    Results stated in Chapter \ref{chap:analysis} have shown that, with the use of either of the two features, a policy can be learned that allows to optimize the order placement when market conditions came in favor of making a purchase or sale.
    However, under the application of either of the two features, the agent performed worse than the expected costs of a market order when conditions were not ideal.
    Moreover, it has been shown that the application of feature II (a sequence of historical trades) results in an overall better policy than the application of feature I (a window of historical order book states).
    \\
    \\
    First, reinforcement learning without market features was tested by using the adapted Q-learning algorithm presented in Section \ref{setup:q-learning}.
    The similar approach presented by Kearns et al. \cite{nevmyvaka2006reinforcement}, and tested on traditional stocks, achieved an optimization in the ranges of 27.16\% to 35.5\% for sell limit orders placed.
    We back-tested, on the cryptocurrency market Bitcoin/USD, both scenarios: buying and selling.
    Interestingly, we achieved similar performance with the Q-Learner, when the market price moved against our favor while attempting to make a sale.
    That is, \$-27.70 was the expected return for selling 1.0 BTC and the agent reduced the return to \$-21.34, e.g. an improvement of 22.96\%.
    The agent was unable to outperform the expected market order costs of \$-1.72 and loss of \$-4.74 was generated.
    For the process of buying, the return improved from \$-1.06 expected to \$-1.04, when the market price was rising and no improvements were achieved when the price moved against our favor--\$-1.17 compared to \$-0.05 expected.
    In general, it can be said that the performance of the Q-learner is relatively close to the costs of a market order, with the one exception where it was significantly more profitable.
    That implies that the absence of market variables has the effect that the agent is able to learn the general principles at which price level to submit orders, however is not able to perform better than a market order.
    The results found for the DQN agent were unlike the ones for the Q-Learning agent. 
    For both market features, the agent was able to improve when market prices allowed, but failed otherwise.
    Hence, the agent learned that rewards are only high for prices below the spread with $a<0$ but was unable to avoid the risk that comes with it.
    The result of which was that the agent optimized purchases, while the market price was falling, from \$-0.05 expected to \$22.06 (feature I) and \$31.92 (feature II).
    A moderate improvement, while making a purchase, was made with feature II: \$-25.15 compared to the expected \$27.70, and performed worse under the application of feature I, that is \$-39.24.
    For the scenario of rising market prices, where the expected market costs were -1.06, no improvements were made for buying: \$-2.26 (feature I) and \$-3.56 (feature II).
    The improvements made for selling were \$-0.84 (feature I) and \$0.15 (feature II) with expected market reward of \$-1.72, that is a smaller improvement than the scenario of buying under the falling market price.
    
    By considering the artificial data sets with linear and sine functions applied, near-optimal performance was achieved with the DQN agent and feature type I (window of historical order book states).
    This confirms that 1) reinforcement learning is indeed capable of optimizing the limit order placement but 2) that there is potential to further improve the setup for real-world market data sets.
    Moreover, the increased optimization capability on a sine shaped order book, compared to the real world market data, indicates that the learner had much difficulty to find a policy under non-stationary conditions and the $C$ parameter (see Section \ref{setup:dqn}) was only of minor assistance.
    Furthermore, it is to be assumed that by 1) increasing the data sets and training epochs as well as 2) increasing the action space in order to consider even more actions to be taken for each step within an epoch, would increase the performance.
    However, this would require significantly more computational resources and time than what was available for this work.
    Likewise, we expect a performance increase when tuning the hyper parameters of the DQN agent with a systematic grid search.
    In addition, during the evaluation of the limitation of the DQN agent we have found that performance is likely to be improved by 1) enlarging the window size of the feature provided to the agent in order to cover long term market movements and 2) by increasing the training epochs in order to make the agent more aware of such pitfalls.

\section{Recommendations and future work}

First and foremost it is recommended to proceed simulations of limit order placement with the setup provided in this work on a live market and therefore make actual sales and purchases.
Thereby, it would be interesting to determine to which extend market participants that approach or leave the market, upon the placement of an order, affect the results found in this work.
By doing so, market fees could be considered and integrated into the reward function provided, followed by an observation in order to determine whether or not the agent adjusts the actions such that market orders, that come with the more expensive \textit{taker fees}, are less often chosen.

An alternative to the live market evaluation would be to make use of an entire artificial market such as proposed by Raberto et al. \cite{raberto2005price}. 
Such a setup would demand multiple agents to continuously post buy and sell orders.
By doing so, some of the agents could act as naive traders and other could make use of the learning capabilities provided in this work.
It would then be interesting to see which category of agents would achieve an overall better performance.

Furthermore, some limitations where discovered in this work and were declared as to be investigated in the future.
This includes the extension of the evaluation procedure in which a the sequence of actions chosen by an agent should be investigated.
The aim of which is to understand what an agent makes to choose for a certain action over another.
Additionally, the model presented and used in the DQN agent provides hyper-parameters which are determined to be further optimized.
A systematic grid search under the application of high computational resources would therefore provide insights whether or not the problem can be further optimized.

In addition, this work could be extended to be a hybrid approach, with the use of imitation learning.
A statistical framework such as proposed in \cite{yingsaeree2012algorithmic} (see Section \ref{sec:related-statistical}) would act as the expert agent, from which an initial policy is learned.
Subsequently, the given deep reinforcement learning mechanisms can be used to learn patterns from the market data that come in favour of the limit order placement.
Thereby, we suggest to update the policy only if the rewards are either significantly positive or negative, in order to prevent from learning noise.

Lastly, the setup built in this work can be used not only to learn order placement but also to learn a market making\cite{o1986microeconomics} strategy.
That is, when buy and sell orders are placed simultaneously with the incentive to make a profit from the difference of the price paid for the buy order and the price received for the sell order.

\section{Summary of contributions}

This work aims to take a step towards answering the important question of how one can optimize the limit order placement on cryptocurrency exchanges using deep reinforcement learning.
Previous work in this field, by Kearns et al., who have studied the behavior of order placement and order execution\cite{nevmyvaka2005electronic} and developed a reinforcement learning strategy\cite{nevmyvaka2006reinforcement} for the purposes of optimization on traditional exchanges, has been extended and applied to the cryptocurrency field.

In this work, raw market data of the Bitcoin/US-Dollar trading pair was analyzed with regard to the activity of market participants.
We have found that patterns emerge while traders create and cancel orders in financial markets.
Based on these findings, two features where designed which incorporate the found patterns in form of a window of 1) historical order book states and 2) historical trades, respectively.
Furthermore, a limit order book data structure as well as a simplified match engine, that enables to emulate a local broker and can match orders using the historical order book, were developed.
Those mechanisms were incorporated into a reinforcement learning environment which allows to make the translation of the order placement problem into the reinforcement learning context.
The environment is configurable and therefore flexible enough to enable investigations with the previously constructed features as well a variety of agents.
In addition, two implementations of reinforcement learning agents were developed: a Q-Learning agent serves as the learner when no market variables are provided and a Deep Q-Network agent is developed to handle the features previously mentioned.

Prior the experimentation with the environment and agents developed, a comprehensive evaluation procedure was worked out as part of this work.
The multi-step procedure involved an empirical investigation of the expected costs of limit and market orders and serves a benchmark for the Q-Learning agent as well as the DQN agent.
The results have shown that the Q-Learning agent was not able to take advantage of market price movements that came in favour of the order placement process.
Nevertheless, the Q-Learner was able to reduce the costs when the market price development did not come in favor of either buying or selling assets and outperformed all considered agent setups, in such a scenario.
The DQN agent, on the other hand, was able to learn a policy that allows to optimize the order placement when market conditions came in favor of making a purchase or sale.
However, this agent performed worse than the expected costs of a market order when conditions were not ideal.
Furthermore, its has been shown that the agent is capable of finding a near-optimal policy on artificial market data, and therefore implies that non-stationary as well as noisy market data prevents the agent from achieving similar results.

This thesis concludes that there is indeed a way in which deep reinforcement learning is capable of optimizing the limit order placement problem.
However, under the application of real world market data, the agent can be exposed to severe conditions, in form of wide spreads or absence of liquidity, which prevent the agent from applying an optimal policy.
Therefore, and in order to constantly be able to perform better or equal placements than what is offered at the financial market, at any given time, the approaches presented in this work have to be further improved and extended.

\section{Application in real world practices}

This work involved a pipeline of tasks, each of which contributed in partly towards answering research questions stated in this thesis.
The tools and components developed throughout this process were thereby consecutively extended and merged.
The result of which is a ready-to-use product, in form of a flexible framework, that allows to implement an intelligent order placement strategy according to the historical or live market data provided by the end user.
The configuration parameters of the environment and agent provided allow to make further adjustments according to the users specific needs.
Moreover, our approach is not limited to cryptocurrency markets but instead supports any product that is traded with limit order books, as long as data is provided.
This in turn makes our setup attractive for numerous of real world practices.
Financial exchanges could use this framework in order to provide a new \textit{order type}, which allows their customers to buy or sell an inventory $I$ of the asset within a time horizon of $H$ seconds.
Moreover, this setup is not restricted to centralized exchanges but instead can also be applied to decentralized exchanges where each node of the exchange would feature its independent agent that learns from the corresponding order book residing in that node.
Another target group of this framework are (institutional) traders or brokers which intend to optimize the way to place orders at the exchanges of their choice.
Therefore, data of the exchange serves as the source of the reinforcement learning environment. 
The agents task is then to act as an \textit{intermediary} between the trader and exchange.